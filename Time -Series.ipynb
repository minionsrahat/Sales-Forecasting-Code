{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dateparse = lambda dates: pd.datetime.strptime(dates, '%d-%m-%Y')\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from statsmodels.tools.eval_measures import rmse \n",
    "from fbprophet import Prophet\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:/Users/RAHAT/Downloads/Untitled Folder/Sales_f/data/Wallmart_Splitted_Train_Data/S__1/S_1__D_4.csv\",parse_dates=['Date'],index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['Store','Dept','IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-05</th>\n",
       "      <td>39954.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-12</th>\n",
       "      <td>35351.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-19</th>\n",
       "      <td>36826.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-26</th>\n",
       "      <td>34660.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-05</th>\n",
       "      <td>38086.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-09-28</th>\n",
       "      <td>34647.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-05</th>\n",
       "      <td>39311.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-12</th>\n",
       "      <td>35446.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-19</th>\n",
       "      <td>35549.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-26</th>\n",
       "      <td>36292.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Weekly_Sales\n",
       "Date                    \n",
       "2010-02-05      39954.04\n",
       "2010-02-12      35351.21\n",
       "2010-02-19      36826.95\n",
       "2010-02-26      34660.16\n",
       "2010-03-05      38086.19\n",
       "...                  ...\n",
       "2012-09-28      34647.33\n",
       "2012-10-05      39311.93\n",
       "2012-10-12      35446.18\n",
       "2012-10-19      35549.19\n",
       "2012-10-26      36292.60\n",
       "\n",
       "[143 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_sarima_params(data):\n",
    "  p = d = q = range(0, 2)\n",
    "  pdq = list(itertools.product(p, d, q))\n",
    "  seasonal_pdq = [(x[0], x[1], x[2], 2) for x in list(itertools.product(p, d, q))]\n",
    "  result_table = pd.DataFrame(columns=['pda','seasonal_pda','aic'])\n",
    "\n",
    "  for param in pdq:\n",
    "      for param_seasonal in seasonal_pdq:\n",
    "          try:\n",
    "            mod = SARIMAX(data,order=param,seasonal_order=param_seasonal,enforce_stationarity=True,enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            result_table = result_table.append({'pda':param, 'seasonal_pda':param_seasonal, 'aic':results.aic},ignore_index=True)\n",
    "          except:\n",
    "            continue\n",
    "\n",
    "  optimal_params = result_table[result_table['aic']==result_table.aic.min()]\n",
    "  print(optimal_params)\n",
    "  print(result_table)\n",
    "  order = optimal_params.pda.values[0]\n",
    "  seasonal_order = optimal_params.seasonal_pda.values[0]\n",
    "  return (order,seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather(df,scaler):\n",
    "    df=df[['temperatureMax']]\n",
    "    scaler=scaler.fit(df)\n",
    "    df['temperatureMax']=scaler.transform(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_grocery(df,scaler):\n",
    "    df.drop(['id','store_nbr','item_nbr','onpromotion'], axis=1,inplace=True)\n",
    "    scaler=scaler.fit(df)\n",
    "    df['unit_sales']=scaler.transform(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_walmart(df,scaler):\n",
    "    df=df.drop(columns=['Store','Dept','IsHoliday'])\n",
    "    scaler=scaler.fit(df)\n",
    "    df['Weekly_Sales']=scaler.transform(df)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_split(df,ratio):\n",
    "    interval=int(len(df)*ratio)\n",
    "    train = df[:interval]\n",
    "    test = df[interval:]\n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "    return train,test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=preprocessing_walmart(df,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=x_y_split(df,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval=int(len(df)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-size 114\n",
      "Tesr-size 29\n"
     ]
    }
   ],
   "source": [
    "train = df[:interval]\n",
    "test = df[interval:]\n",
    "train['Weekly_Sales']=scaler.transform(train)\n",
    "test['Weekly_Sales']=scaler.transform(test)\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "start = len(train) \n",
    "end = len(train) + len(test) - 1\n",
    "print('Train-size', len(train))\n",
    "print('Tesr-size', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pda  seasonal_pda          aic\n",
      "46  (1, 0, 1)  (1, 1, 0, 2)  2133.706834\n",
      "          pda  seasonal_pda          aic\n",
      "0   (0, 0, 0)  (0, 0, 0, 2)  2722.714421\n",
      "1   (0, 0, 0)  (0, 0, 1, 2)  2650.047775\n",
      "2   (0, 0, 0)  (0, 1, 0, 2)  2173.594522\n",
      "3   (0, 0, 0)  (0, 1, 1, 2)  2152.333570\n",
      "4   (0, 0, 0)  (1, 0, 0, 2)  2223.205930\n",
      "..        ...           ...          ...\n",
      "59  (1, 1, 1)  (0, 1, 1, 2)  2155.972557\n",
      "60  (1, 1, 1)  (1, 0, 0, 2)  2155.438886\n",
      "61  (1, 1, 1)  (1, 0, 1, 2)  2153.294815\n",
      "62  (1, 1, 1)  (1, 1, 0, 2)  2156.826843\n",
      "63  (1, 1, 1)  (1, 1, 1, 2)  2157.742346\n",
      "\n",
      "[64 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "order,seasonal_order = get_sarima_params(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(train,order=order)\n",
    "model_fit = model.fit()\n",
    "predictions =model_fit.predict(start, end, typ = 'levels').rename(\"Predictions\")\n",
    "test['ARIMA']=np.array((predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    }
   ],
   "source": [
    "train_prophet=pd.DataFrame()\n",
    "train_prophet['ds']=train.index\n",
    "train_prophet['y']=np.array(train['Weekly_Sales'])\n",
    "m1 = Prophet(weekly_seasonality=True)\n",
    "m1.fit(train_prophet)\n",
    "future1 = m1.make_future_dataframe(periods=len(test.index),freq='D')\n",
    "forecast1 = m1.predict(future1)\n",
    "pred_prophet=(((forecast1[['yhat']])))\n",
    "p=pred_prophet.iloc[interval:]\n",
    "test['PROPHET']=np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARIMA\n",
    "model = SARIMAX(train, order=order,seasonal_order=seasonal_order, enforce_stationarity=False,enforce_invertibility=False)\n",
    "model_fit = model.fit(disp=False)\n",
    "# make prediction\n",
    "yhat= model_fit.predict(start,end)\n",
    "test['SARIMAX']=np.array(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = ExponentialSmoothing(np.asarray(train) ,seasonal_periods=len(test), seasonal='add').fit(optimized=True, use_brute=True)\n",
    "#y_hat_avg['Holt_Winter'] = fit1.forecast(len(test_data))\n",
    "predictions_holt_winter = fit1.predict(start, end)\n",
    "                            \n",
    "test['HOLT_WINTER']=np.array(predictions_holt_winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = len(test)\n",
    "alpha = 2/(span+1)\n",
    "model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n",
    "test_predictions = model.forecast(len(test))\n",
    "test['SES']=np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pda  seasonal_pda         aic\n",
      "21  (0, 1, 0)  (1, 0, 1, 2) -170.782742\n",
      "          pda  seasonal_pda         aic\n",
      "0   (0, 0, 0)  (0, 0, 0, 2)  138.699672\n",
      "1   (0, 0, 0)  (0, 0, 1, 2)   52.262300\n",
      "2   (0, 0, 0)  (0, 1, 0, 2) -115.333056\n",
      "3   (0, 0, 0)  (0, 1, 1, 2) -118.679159\n",
      "4   (0, 0, 0)  (1, 0, 0, 2) -111.138922\n",
      "..        ...           ...         ...\n",
      "59  (1, 1, 1)  (0, 1, 1, 2) -153.192418\n",
      "60  (1, 1, 1)  (1, 0, 0, 2) -167.909490\n",
      "61  (1, 1, 1)  (1, 0, 1, 2) -168.202776\n",
      "62  (1, 1, 1)  (1, 1, 0, 2) -126.120100\n",
      "63  (1, 1, 1)  (1, 1, 1, 2) -151.312560\n",
      "\n",
      "[64 rows x 3 columns]\n",
      "Armenia.csv\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "from math import sqrt\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "# paths_to_folders = ['C:/Users/RAHAT/Downloads/Untitled Folder/Sales_f/data/Wallmart_Splitted_Train_Data/S__5']\n",
    "paths_to_folders = ['C:/Users/RAHAT/Downloads/Untitled Folder/Sales_f/data/Practice/weather']\n",
    "x=0\n",
    "error = pd.DataFrame(columns=['File_name', 'ARIMA_RMSE','HOLT_WINTER_RMSE','PROPHET_RMSE','SES_RMSE','SARIMA_RMSE',\n",
    "                              'ARIMA_MAPE_ERROR','HOLT_WINTER_MAPE_ERROR','PROPHET_MAPE_ERROR','SES_MAPE_ERROR','SARIMA_MAPE_ERROR'])\n",
    "for folder in paths_to_folders:\n",
    "   for csv_file in os.listdir(folder):\n",
    "     df=pd.read_csv(\"C:/Users/RAHAT/Downloads/Untitled Folder/Sales_f/data/Practice/weather/\"+csv_file,parse_dates=['time'],index_col='time')\n",
    "     #print(len(df.index);\n",
    "     if len(df.index)>=100:\n",
    "         interval=int(len(df)*0.8)\n",
    "         df=preprocess_weather(df,scaler)\n",
    "         train,test=x_y_split(df,0.8)\n",
    "         start = len(train) \n",
    "         end = len(train) + len(test) - 1\n",
    "         warnings.filterwarnings(\"ignore\")\n",
    "         order,seasonal_order = get_sarima_params(train)\n",
    "        \n",
    "         #ARIMA MODEL\n",
    "         \n",
    "         model = ARIMA(train,order=order)\n",
    "         model_fit = model.fit()\n",
    "         predictions =model_fit.predict(start, end, typ = 'levels').rename(\"Predictions\")\n",
    "         test['ARIMA']=np.array((predictions))\n",
    "            \n",
    "        \n",
    "         #Prophet Model  \n",
    "         train_prophet=pd.DataFrame()\n",
    "         train_prophet['ds']=train.index\n",
    "         train_prophet['y']=np.array(train['temperatureMax'])\n",
    "         m1 = Prophet(weekly_seasonality=True)\n",
    "         m1.fit(train_prophet)\n",
    "         future1 = m1.make_future_dataframe(periods=len(test.index),freq='D')\n",
    "         forecast1 = m1.predict(future1)\n",
    "         pred_prophet=(((forecast1[['yhat']])))\n",
    "         p=pred_prophet.iloc[interval:]\n",
    "         test['PROPHET']=np.array(p)\n",
    "            \n",
    "         #SARIMA MODEL\n",
    "         model = SARIMAX(train, order=order,seasonal_order=seasonal_order, enforce_stationarity=False,enforce_invertibility=False)\n",
    "         model_fit = model.fit(disp=False)\n",
    "         # make prediction\n",
    "         yhat= model_fit.predict(start,end)\n",
    "         test['SARIMA']=np.array(yhat)\n",
    "\n",
    "            \n",
    "         #HOLT WINTER\n",
    "         fit1 = ExponentialSmoothing(np.asarray(train) ,seasonal_periods=len(test), seasonal='add').fit(optimized=True, use_brute=True)\n",
    "         #y_hat_avg['Holt_Winter'] = fit1.forecast(len(test_data))\n",
    "         predictions_holt_winter = fit1.predict(start, end)\n",
    "         test['HOLT_WINTER']=np.array(predictions_holt_winter)\n",
    "        \n",
    "         #SES\n",
    "         span = len(test)\n",
    "         alpha = 2/(span+1)\n",
    "         model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n",
    "         test_predictions = model.forecast(len(test))\n",
    "         test['SES']=np.array(test_predictions)\n",
    "        \n",
    "        \n",
    "         error.loc[x, ['File_name']]=csv_file\n",
    "         error.loc[x, ['ARIMA_RMSE']]=rmse(test['temperatureMax'],test['ARIMA'])\n",
    "         error.loc[x, ['SES_RMSE']]=rmse(test['temperatureMax'],test['SES'])\n",
    "         error.loc[x, ['SARIMA_RMSE']]=rmse(test['temperatureMax'],test['SARIMA'])\n",
    "         error.loc[x, ['PROPHET_RMSE']]=rmse(test['temperatureMax'],test['PROPHET'])\n",
    "         error.loc[x, ['HOLT_WINTER_RMSE']]=rmse(test['temperatureMax'],test['HOLT_WINTER'])\n",
    "        \n",
    "        \n",
    "         error.loc[x, ['ARIMA_MAPE_ERROR']]=mean_absolute_percentage_error(test['temperatureMax'],test['ARIMA'])\n",
    "         error.loc[x, ['SES_MAPE_ERROR']]=mean_absolute_percentage_error(test['temperatureMax'],test['SES'])\n",
    "         error.loc[x, ['SARIMA_MAPE_ERROR']]=mean_absolute_percentage_error(test['temperatureMax'],test['SARIMA'])\n",
    "         error.loc[x, ['PROPHET_MAPE_ERROR']]=mean_absolute_percentage_error(test['temperatureMax'],test['PROPHET'])\n",
    "         error.loc[x, ['HOLT_WINTER_MAPE_ERROR']]=mean_absolute_percentage_error(test['temperatureMax'],test['HOLT_WINTER'])\n",
    "    \n",
    "         x=x+1\n",
    "         print(csv_file)\n",
    "         print(x)\n",
    "         #print(rmse(test[\"unit_sales\"], predictions))\n",
    "         #print(mean_squared_error(test[\"unit_sales\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>ARIMA_RMSE</th>\n",
       "      <th>HOLT_WINTER_RMSE</th>\n",
       "      <th>PROPHET_RMSE</th>\n",
       "      <th>SES_RMSE</th>\n",
       "      <th>SARIMA_RMSE</th>\n",
       "      <th>ARIMA_MAPE_ERROR</th>\n",
       "      <th>HOLT_WINTER_MAPE_ERROR</th>\n",
       "      <th>PROPHET_MAPE_ERROR</th>\n",
       "      <th>SES_MAPE_ERROR</th>\n",
       "      <th>SARIMA_MAPE_ERROR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Armenia.csv</td>\n",
       "      <td>0.167742</td>\n",
       "      <td>0.150409</td>\n",
       "      <td>0.119524</td>\n",
       "      <td>0.132093</td>\n",
       "      <td>0.144902</td>\n",
       "      <td>16.6143</td>\n",
       "      <td>14.2572</td>\n",
       "      <td>15.0944</td>\n",
       "      <td>13.8045</td>\n",
       "      <td>14.8725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_name ARIMA_RMSE HOLT_WINTER_RMSE PROPHET_RMSE  SES_RMSE SARIMA_RMSE  \\\n",
       "0  Armenia.csv   0.167742         0.150409     0.119524  0.132093    0.144902   \n",
       "\n",
       "  ARIMA_MAPE_ERROR HOLT_WINTER_MAPE_ERROR PROPHET_MAPE_ERROR SES_MAPE_ERROR  \\\n",
       "0          16.6143                14.2572            15.0944        13.8045   \n",
       "\n",
       "  SARIMA_MAPE_ERROR  \n",
       "0           14.8725  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "error.to_csv('weather_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "from math import sqrt\n",
    "paths_to_folders = ['C:/Users/RAHAT/Downloads/Untitled Folder/Sales_f/data/Wallmart_Splitted_Train_Data/S__5']\n",
    "x=0\n",
    "error = pd.DataFrame(columns=['File_name','DNN_RMSE','DNN_MAPE','CNN_RMSE','CNN_MAPE','LSTM_RMSE','LSTM_MAPE'])\n",
    "for folder in paths_to_folders:\n",
    "    for csv_file in os.listdir(folder):\n",
    "        df=pd.read_csv(\"C:/Users/RAHAT/Downloads/Untitled Folder/Sales_f/data/Wallmart_Splitted_Train_Data/S__5/\"+csv_file,parse_dates=['Date'],index_col='Date')\n",
    "        #print(len(df.index))\n",
    "        df=df.drop(columns=['Store','Dept','IsHoliday'])\n",
    "        if len(df.index)>=140:\n",
    "             i=int(len(df.index)*0.8)\n",
    "             train = df.iloc[0:i]\n",
    "             test = df.iloc[i:]\n",
    "             train_scal=pd.DataFrame(scaler.fit_transform(pd.DataFrame(train['Weekly_Sales'])))\n",
    "             test_scal=pd.DataFrame(scaler.fit_transform(pd.DataFrame(test['Weekly_Sales'])))\n",
    "             train_scal=train_scal.replace(0, np.nan)\n",
    "             train_scal = train_scal.dropna()\n",
    "             test_scal=test_scal.replace(0, np.nan)\n",
    "             test_scal = test_scal.dropna()\n",
    "             \n",
    "             xtrain,ytrain=convert2matrix(train_scal.values,4)\n",
    "             xtest,ytest=convert2matrix(test_scal.values,4)\n",
    "             #DNN MODEL\n",
    "             model=model_dnn(4)\n",
    "             history=model.fit(xtrain,ytrain, epochs=60, batch_size=32, verbose=1,callbacks=[EarlyStopping(monitor='val_loss', patience=10)],shuffle=False)\n",
    "             test_predict = model.predict(xtest)\n",
    "             \n",
    "        \n",
    "             error.loc[x, ['File_name']]=csv_file\n",
    "             error.loc[x, ['DNN_RMSE']]=np.sqrt(mean_squared_error(ytest,test_predict))\n",
    "             error.loc[x, ['DNN_MAPE']]=mean_absolute_percentage_error(ytest,test_predict)\n",
    "             \n",
    "             xtrain=xtrain.reshape(xtrain.shape[0],xtrain.shape[1],1)\n",
    "             xtest=xtest.reshape(xtest.shape[0],xtest.shape[1],1)\n",
    "\n",
    "             #CNN MODEL\n",
    "             model = Sequential()\n",
    "             model.add(Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(xtrain.shape[1], 1)))\n",
    "             model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "             model.add(Flatten())\n",
    "             model.add(Dense(100, activation='relu'))\n",
    "             model.add(Dense(1))\n",
    "             model.compile(optimizer='adam', loss='mse')\n",
    "             # fit model\n",
    "             model.fit(xtrain, ytrain, epochs=60, verbose=1)\n",
    "             cnn_preds = model.predict(xtest)\n",
    "             \n",
    "\n",
    "             error.loc[x, ['CNN_RMSE']]=np.sqrt(mean_squared_error(ytest,cnn_preds))\n",
    "             error.loc[x, ['CNN_MAPE']]=mean_absolute_percentage_error(ytest,cnn_preds)\n",
    "                \n",
    "             \n",
    "             #LSTM_MODEL\n",
    "             model = Sequential()\n",
    "             model.add(LSTM(100, return_sequences=True, input_shape=(xtrain.shape[1], xtrain.shape[2])))\n",
    "             model.add(Dropout(0.2))\n",
    "             model.add(LSTM(50))\n",
    "             model.add(Dropout(0.1))\n",
    "             model.add(Dense(1))\n",
    "             model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "             # fit network\n",
    "             model.fit(xtrain, ytrain, epochs=60, batch_size=32,  verbose=1, shuffle=False)\n",
    "             lstm_preds = model.predict(xtest)\n",
    "\n",
    "             error.loc[x, ['LSTM_RMSE']]=np.sqrt(mean_squared_error(ytest,lstm_preds))\n",
    "             error.loc[x, ['LSTM_MAPE']]=mean_absolute_percentage_error(ytest,lstm_preds)\n",
    "\n",
    "             x=x+1\n",
    "             print(csv_file+\"\\n\")\n",
    "             print(x)\n",
    "             #print(rmse(test[\"unit_sales\"], predictions))\n",
    "             #print(mean_squared_error(test[\"unit_sales\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
